# Fine-tune-Vision-Language-Model

This repository contains the implementation of the Vision-and-Language Transformer (ViLT) model fine-tuned for Visual Question Answering (VQA) tasks. The project is structured to be easy to set up and use, providing a streamlined approach for experimenting with different configurations and datasets.


![Screenshot_2024-08-08_at_9 50 25_PM-removebg-preview](https://github.com/user-attachments/assets/9d82bbb9-814b-4336-bf7b-efba7e19b8d9)



## Installation

1. **Clone the Repository**
```bash
git clone https://your-repository-url.git
cd vilt-vqa
```


2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

